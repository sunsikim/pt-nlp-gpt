{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "462d0ff2-5bcc-4e07-aabc-2e4261f157ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pathlib\n",
    "import sys\n",
    "import torch\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from jobs.configure import GPT2Config\n",
    "from gpt.model import GPT2Attention\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4067ebe-1b8a-4b91-89ef-32ffddc368d3",
   "metadata": {},
   "source": [
    "For simplicity, configure the size of attention layer small as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36b8db2a-4491-413a-bcc9-e005fa2f6a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Attention(\n",
       "  (c_attn): Linear(in_features=12, out_features=36, bias=False)\n",
       "  (c_proj): Linear(in_features=12, out_features=12, bias=False)\n",
       "  (attn_dropout): Dropout(p=0.2, inplace=False)\n",
       "  (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 16  # denote as B\n",
    "token_count = 5  # denote as T\n",
    "embed_size = 12  # denote as S\n",
    "num_heads = 3    # denote as H\n",
    "\n",
    "cfg = GPT2Config(\n",
    "    block_size=token_count,\n",
    "    n_embd=embed_size,\n",
    "    n_head=num_heads,\n",
    ")\n",
    "layer = GPT2Attention(cfg)\n",
    "layer.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341f603e-c23a-4160-9a8d-52b261113503",
   "metadata": {},
   "source": [
    "Batch of token_ids will have shape of (B, T). If passed to embedding layer, resulting tensor will have shape of (B, T, S) illustrated as above. This causal attention layer will map this tensor into a new tensor of shape (B, T, S) after forward computation. This notebook will demonstrate the steps of this computation in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor = torch.randn(batch_size, cfg.block_size, cfg.n_embd)  # (B, T, S)\n",
    "B, T, S = input_tensor.size()\n",
    "original_output = layer(input_tensor)\n",
    "input_tensor.size() == original_output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a990d91-4397-4207-bde7-43c8212671f3",
   "metadata": {},
   "source": [
    "![input tensor](./images/causal_attention/input_tensor.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772ea80f-85c0-43d8-b407-c0506f06ee1f",
   "metadata": {},
   "source": [
    "Dot product with weight matrix `c_attn` is calculated to map `input_tensor` into query, key, value tensor respsectively. Again, for simplicity, let's zoom into first tensor of a input_tensor only(i.e. `input_tensor[0]`) from now on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "929c8f60-1c37-497d-81a3-d7714e25c872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q, k, v = layer.c_attn(input_tensor).split(S, dim=2)\n",
    "q.size() == k.size() == v.size() == input_tensor.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a366db-2b70-4d47-9a93-31b66065f4b9",
   "metadata": {},
   "source": [
    "![qkv split](./images/causal_attention/qkv_split.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9393fe-300c-499d-b04a-51eeccf57d41",
   "metadata": {},
   "source": [
    "Each of query, key, value tensor is reshaped into tensor of shape (B, H, T, S // H), to represent input for each head in multi head attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e2a6613-1d60-42a8-9608-e314706c5ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 5, 4])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = q.view(B, T, layer.num_heads, layer.head_dim).transpose(1, 2)\n",
    "k = k.view(B, T, layer.num_heads, layer.head_dim).transpose(1, 2)\n",
    "v = v.view(B, T, layer.num_heads, layer.head_dim).transpose(1, 2)\n",
    "q.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efb59de-49ff-4b7f-997d-5499410a81f0",
   "metadata": {},
   "source": [
    "![qkv split](./images/causal_attention/multihead_view.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713f1e7a-42f0-40e4-b04e-5ee1f5f7d78d",
   "metadata": {},
   "source": [
    "Then, to ensure only the attention of $i+k$-th tokens is applied on $i$-th token:\n",
    "1. dot product between query and key are calculated(logits)\n",
    "2. upper triangular elements of this dot product are replaced to smallest value(-inf)\n",
    "3. softmax function is applied, to make this replaced value will result in 0 softmax value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08a28ec0-ecc0-4c6a-b97c-3bc72a9d44c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 5, 4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))     # scaled dot product\n",
    "att = att.masked_fill(layer.bias[:, :, :T, :T] == 0, float(\"-inf\")) # masked_fill\n",
    "att = F.softmax(att, dim=-1)\n",
    "y = att @ v\n",
    "y.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f001f59-f105-4766-8b41-ba04ebf71a46",
   "metadata": {},
   "source": [
    "![qkv split](./images/causal_attention/masked_scaled_dot_product.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6dcb12-e1a0-4dd2-a6c7-aeeab228d7fb",
   "metadata": {},
   "source": [
    "Since resulting tensor `y` is still separated into H tensors, it has to be combined into single tensor. Note that `y` seemingly multiple tensors in this illustration, but as mentioned, it is actually (B, H, T, S // H) shaped single tensor. That is, it cannot be simply reshaped using `torch.stack` or `torch.cat`, therefore has to be manipulated by complex view arrangement as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f606119-e20b-4daf-baff-cfa0c0262de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 5, 12])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = y.transpose(1, 2).contiguous().view(B, T, S)\n",
    "y.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7682ee34-c707-43dc-8fae-6d738cf233b8",
   "metadata": {},
   "source": [
    "![qkv split](./images/causal_attention/reshape.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70f8a30-cabc-4168-bed3-a18596584d30",
   "metadata": {},
   "source": [
    "After additional matrix multiplication with `c_proj` linear layer, this tensor becomes output of causal attention layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46c8e8e3-b2ad-4780-b7cf-810cf445be7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(layer.c_proj(y), original_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
