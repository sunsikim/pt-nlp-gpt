{
  "model": {
    "block_size": 1024
  },
  "optimizer": {
    "weight_decay": 1e-1
  },
  "trainer": {
    "wandb_log": true,
    "wandb_project": "owt",
    "wandb_run_name": "gpt2-124M",
    "batch_size": 12,
    "gradient_accumulation_steps": 40,
    "max_iters": 600000,
    "eval_interval": 1000,
    "eval_iters": 200,
    "log_interval": 10
  }
}